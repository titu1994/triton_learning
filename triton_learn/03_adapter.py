"""
export TORCH_COMPILE_DEBUG=1

Run script with uncommented pytorch code to get initial codebase first.
"""

import torch
import torch._dynamo as dynamo

from nemo.core.classes.mixins import adapter_mixins, adapter_mixin_strategies
from nemo.collections.common.parts.adapter_modules import LinearAdapterConfig
from torch.nn.modules import Module

dynamo.reset()

# Hack for a triton bug - https://github.com/pytorch/pytorch/issues/124565
# torch.empty(1, device='cuda', requires_grad=True).backward()


class MLPNN(torch.nn.Module, adapter_mixins.AdapterModuleMixin):

    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLPNN, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim

        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)
        self.act = torch.nn.ReLU()

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)

        if self.is_adapter_available():
            x = self.forward_enabled_adapters(x)
        return x
    
    def forward_enabled_adapters(self, input: 'torch.Tensor'):
        """
        Implement parallel adapters in the forward pass.

        Args:
            input: The output tensor of the calling module is the input to the first adapter, whose output
                is then chained to the next adapter until all adapters are consumed.

        Returns:
            The result tensor, after all active adapters have finished their forward passes, summed up with the 
            original output.
        """
        enabled_adapters = self.get_enabled_adapters()
        adapter_strategy = self.adapter_layer[enabled_adapters[0]].adapter_strategy

        return self.forward_single_enabled_adapter_(input, self.adapter_layer, adapter_strategy)
        
    
    def forward_single_enabled_adapter_(self, input: torch.Tensor, adapter_modules: torch.nn.ModuleDict, strategy):
        original_input = input
        outputs = []
        for adapter_name, adapter_module in adapter_modules.items():
            # Call a single adapter's forward, and accept its output as the new input for the next adapter.
            output = super().forward_single_enabled_adapter_(
                input, adapter_module, adapter_name=adapter_name, adapter_strategy=strategy
            )
            outputs.append(output)
            
        return original_input + sum(outputs)

    

torch.manual_seed(0)

in_features = 10
out_features = 5
hidden_dim = 16
adapter_dim = 2
adapter_1_cfg = LinearAdapterConfig(in_features=out_features, dim=adapter_dim)
adapter_2_cfg = LinearAdapterConfig(in_features=out_features, dim=adapter_dim)

model = MLPNN(in_features, hidden_dim, out_features)

model.forward_single_enabled_adapter_ = torch.compile(model.forward_single_enabled_adapter_, dynamic=True, mode="reduce-overhead")

model.add_adapter('adapter1', adapter_1_cfg)
model.add_adapter('adapter2', adapter_2_cfg)

model = model.cuda()

dummy_input = torch.randn(3, in_features, device='cuda')
dummy_output = model(dummy_input)

print("Model: ", model)

# def forward_backward(x, y):
#     # Compute forward kernel
#     c = add(x, y)
#     # Compute backward kernel
#     c.backward(torch.ones_like(c))
#     return c

# op = torch.compile(add, dynamic=True, mode="default", disable=True)
# op_grad = torch.compile(forward_backward, dynamic=True, mode="default")

# x = torch.ones(2, 3, device="cuda", requires_grad=True) * 3
# y = torch.ones(2, 3, device="cuda") * 2

# z = op(x, y)
# _ = op_grad(x, y)

# print(z)


"""
Copy the triton kernels generated by Pytorch
"""
