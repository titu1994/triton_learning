"""
export TORCH_COMPILE_DEBUG=1

Run script with uncommented pytorch code to get initial codebase first.
"""

# import torch
# import torch._dynamo as dynamo

# dynamo.reset()

# # Hack for a triton bug - https://github.com/pytorch/pytorch/issues/124565
# torch.empty(1, device='cuda', requires_grad=True).backward()

# def add(a, b):
#     return a + b

# def forward_backward(x, y):
#     # Compute forward kernel
#     c = add(x, y)
#     # Compute backward kernel
#     c.backward(torch.ones_like(c))
#     return c

# op = torch.compile(add, dynamic=True, mode="default", disable=True)
# op_grad = torch.compile(forward_backward, dynamic=True, mode="default")

# x = torch.ones(2, 3, device="cuda", requires_grad=True) * 3
# y = torch.ones(2, 3, device="cuda") * 2

# z = op(x, y)
# _ = op_grad(x, y)

# print(z)


"""
Copy the triton kernels generated by Pytorch
"""

import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

import torch
from torch._inductor import triton_helpers, triton_heuristics

# @triton_heuristics.pointwise(
#     size_hints=[8], 
#     filename=__file__,
#     triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
#     inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_0', 'mutated_arg_names': [], 'no_x_dim': False, 'backend_hash': 'a94de9f8764f8451f2c631e6c99ecb064c321e0980c2186fd5f1af32f74c5209'},
#     min_elem_per_thread=0
# )
@triton.jit
def pointwise_add_fwd(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tl.load(in_ptr1 + (x0), xmask)
    tmp2 = tmp0 + tmp1
    tl.store(out_ptr0 + (x0), tmp2, xmask)


# @triton_heuristics.pointwise(
#     size_hints=[8], 
#     filename=__file__,
#     triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
#     inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_0', 'mutated_arg_names': [], 'no_x_dim': False, 'backend_hash': 'a94de9f8764f8451f2c631e6c99ecb064c321e0980c2186fd5f1af32f74c5209'},
#     min_elem_per_thread=0
# )
@triton.jit
def pointwise_add_bwd(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = 1.0
    tmp2 = tmp0 * tmp1
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def exec_forward(x, y):
    with torch.cuda.device(0):
        buffer = torch.empty_like(x)
        nelem = buffer.numel()
        stream = torch.cuda.Stream()
        grid = lambda meta: (triton.cdiv(nelem, meta['XBLOCK']),)

        with torch.cuda.stream(stream):
            pointwise_add_fwd[grid](x, y, buffer, nelem, XBLOCK=1024)
        
        torch.cuda.synchronize()
        del x
        del y
    return buffer


def exec_backward(grad_output):
    with torch.cuda.device(0):
        # Source Nodes: [], Original ATen: [aten.mul]
        buffer = torch.empty_like(x)
        numel = grad_output.numel()
        stream = torch.cuda.Stream()
        grid = lambda meta: (triton.cdiv(numel, meta['XBLOCK']),)

        with torch.cuda.stream(stream):
            pointwise_add_bwd[grid](grad_output, buffer, numel, XBLOCK=1024)
            del grad_output
        
    return buffer


if __name__ == "__main__":
    x = torch.ones(2, 3, device="cuda", requires_grad=True) * 3
    y = torch.ones(2, 3, device="cuda") * 2

    print(exec_forward(x, y))

    grad_output = torch.ones(2, 3, device="cuda")
    print(exec_backward(grad_output))
