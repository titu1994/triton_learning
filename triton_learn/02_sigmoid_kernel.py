"""
export TORCH_COMPILE_DEBUG=1

Run script with uncommented pytorch code to get initial codebase first.
"""

# import torch
# import torch._dynamo as dynamo

# dynamo.reset()

# # Hack for a triton bug - https://github.com/pytorch/pytorch/issues/124565
# torch.empty(1, device='cuda', requires_grad=True).backward()

# def sigmoid(a):
#     return (1. / (1. + torch.exp(-a)))

# def forward_backward(x):
#     # Compute forward kernel
#     y = sigmoid(x)
#     # Compute backward kernel
#     y.backward(torch.ones_like(y))
#     return y

# op = torch.compile(sigmoid, dynamic=True, mode="default", disable=True)
# op_grad = torch.compile(forward_backward, dynamic=True, mode="default")

# x = torch.rand(2, 3, device="cuda", requires_grad=True)

# z = op(x)
# _ = op_grad(x)

# print(z)


"""
Copy the triton kernels generated by Pytorch
"""

import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

import torch
from torch._inductor import triton_helpers, triton_heuristics
from torch._inductor.triton_helpers import libdevice, math as tl_math

# @triton_heuristics.pointwise(
#     size_hints=[8], 
#     filename=__file__,
#     triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
#     inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_exp_mul_neg_reciprocal_0', 'mutated_arg_names': [], 'no_x_dim': False, 'backend_hash': 'a94de9f8764f8451f2c631e6c99ecb064c321e0980c2186fd5f1af32f74c5209'},
#     min_elem_per_thread=0
# )
@triton.jit
def pointwise_sigmoid_fwd(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = -tmp0
    tmp2 = tl_math.exp(tmp1)
    tmp3 = 1.0
    tmp4 = tmp2 + tmp3
    tmp5 = 1 / tmp4
    tmp6 = tmp5 * tmp3
    tl.store(out_ptr0 + (x0), tmp6, xmask)


# """
# Original inefficient backward kernel
# """
# # @triton_heuristics.pointwise(
# #     size_hints=[8], 
# #     filename=__file__,
# #     triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
# #     inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_exp_mul_neg_reciprocal_0', 'mutated_arg_names': [], 'no_x_dim': False, 'backend_hash': 'a94de9f8764f8451f2c631e6c99ecb064c321e0980c2186fd5f1af32f74c5209'},
# #     min_elem_per_thread=0
# # )
# @triton.jit
# def pointwise_sigmoid_bwd_generated(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
#     xoffset = tl.program_id(0) * XBLOCK
#     xindex = xoffset + tl.arange(0, XBLOCK)[:]
#     xmask = xindex < xnumel
#     x0 = xindex
#     tmp0 = tl.load(in_ptr0 + (x0), xmask)
#     tmp4 = tl.load(in_ptr1 + (x0), xmask)
#     tmp1 = 1.0
#     tmp2 = tmp0 * tmp1
#     tmp3 = -tmp2
#     tmp5 = -tmp4
#     tmp6 = tl_math.exp(tmp5)
#     tmp7 = tmp6 + tmp1
#     tmp8 = 1 / tmp7
#     tmp9 = tmp8 * tmp8
#     tmp10 = tmp3 * tmp9
#     tmp11 = tmp10 * tmp6
#     tmp12 = -tmp11
#     tl.store(out_ptr0 + (x0), tmp12, xmask)


"""
Efficeint kernel using cached forward pass output (grad(sigmoid(x))) = sigmoid(x) * (1 - sigmoid(x))
"""
# @triton_heuristics.pointwise(
#     size_hints=[8], 
#     filename=__file__,
#     triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
#     inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_exp_mul_neg_reciprocal_0', 'mutated_arg_names': [], 'no_x_dim': False, 'backend_hash': 'a94de9f8764f8451f2c631e6c99ecb064c321e0980c2186fd5f1af32f74c5209'},
#     min_elem_per_thread=0
# )
@triton.jit
def pointwise_sigmoid_bwd_efficient(cached_sigmoid, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    sigmoid_x = tl.load(cached_sigmoid + (x0), xmask)
    tmp = tl.load(out_ptr0 + (x0), xmask)
    grad = sigmoid_x * (1 - sigmoid_x)
    tmp2 = grad * tmp
    tl.store(out_ptr0 + (x0), tmp2, xmask)



def exec_forward(x):
    with torch.cuda.device(0):
        buffer = torch.empty_like(x)
        nelem = buffer.numel()
        stream = torch.cuda.Stream()
        grid = lambda meta: (triton.cdiv(nelem, meta['XBLOCK']),)

        with torch.cuda.stream(stream):
            pointwise_sigmoid_fwd[grid](x, buffer, nelem, XBLOCK=1024)
        
        torch.cuda.synchronize()
        del x
    return buffer


def exec_backward(sigmoid_out, grad_output):
    with torch.cuda.device(0):
        # Source Nodes: [], Original ATen: [aten.mul]
        buffer = torch.empty_like(x)
        numel = grad_output.numel()
        stream = torch.cuda.Stream()
        grid = lambda meta: (triton.cdiv(numel, meta['XBLOCK']),)

        with torch.cuda.stream(stream):
            pointwise_sigmoid_bwd_efficient[grid](sigmoid_out, buffer, numel, XBLOCK=1024)
            del grad_output
        
    return buffer


if __name__ == "__main__":
    x = torch.randn(2, 3, device="cuda", requires_grad=True)
    sigmoid_x = exec_forward(x)
    print(sigmoid_x)

    grad_output = torch.ones(2, 3, device="cuda")
    print(exec_backward(sigmoid_x, grad_output))
